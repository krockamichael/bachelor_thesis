encoder layers: non-trainable
LSTM: 2
neurons_1: 128
neurons_2: 64.0
epochs: 20
autoencoder batch_size: 128
n_clusters: 7
clusters batch_size: 256
max iterations: 8000
tolerance threshold: 0.001
regularizers.l1(10e-5): LSTM layer 2
Autoencoder training time: 3597.2797281999997
Clustering training time: 10021.494345903397
0    0.167703
1    0.156334
2    0.156496
3    0.156717
4    0.158300
5    0.165909
6    0.166797
dtype: float64
3    11075
0     6007
2      785
6      216
4      114
5       51
Name: 7, dtype: int64
