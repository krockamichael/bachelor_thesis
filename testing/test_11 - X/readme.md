encoder layers: non-trainable
LSTM: 2
neurons_1: 128
neurons_2: 64.0
epochs: 20
autoencoder batch_size: 128
n_clusters: 7
clusters batch_size: 256
max iterations: 8000
tolerance threshold: 0.001
regularizers.l1(10e-5): LSTM layer 2
dropout(0.2): LSTM layer 1
Clustering training time: 9549.280062913895
0    0.176662
1    0.159092
2    0.159392
3    0.186771
4    0.163440
5    0.161974
6    0.159727
dtype: float64
1    4941
2    3890
0    3195
5    2253
6    1948
4    1905
3     116
Name: 7, dtype: int64
