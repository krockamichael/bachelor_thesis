encoder layers: non-trainable
LSTM: 2
neurons_1: 128
neurons_2: 64.0
epochs: 20
autoencoder batch_size: 128
n_clusters: 10
clusters batch_size: 256
max iterations: 8000
tolerance threshold: 0.001
regularizers.l1(10e-5): LSTM layer 2
Autoencoder training time: 3657.3083533999998
Clustering training time: 10036.766239643097
0    0.108665
1    0.125879
2    0.107350
3    0.125142
4    0.108207
5    0.110325
6    0.122633
7    0.110183
8    0.126093
9    0.111265
dtype: float64
8    11329
0     3414
6     1761
4      806
2      721
9      211
5        6
Name: 10, dtype: int64
