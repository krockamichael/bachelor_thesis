encoder layers: non-trainable
LSTM: 2
neurons_1: 128
neurons_2: 64.0
epochs: 20
autoencoder batch_size: 128
n_clusters: 10
clusters batch_size: 256
max iterations: 8000
tolerance threshold: 0.001
regularizers.l1(10e-5): LSTM layer 2
iteration: 140
delta_label: 0.0
Clustering training time: 307.94290804862976
0    0.102584
1    0.104290
2    0.113175
3    0.102952
4    0.105779
5    0.103498
6    0.105598
7    0.105264
8    0.105563
9    0.106875
dtype: float64
0    9210
3    2521
5    1993
1    1132
6     772
7     739
4     698
8     529
9     517
2     137
Name: 10, dtype: int64
