encoder layers: non-trainable
LSTM: 2
neurons_1: 128
neurons_2: 64.0
epochs: 20
autoencoder batch_size: 128
n_clusters: 200
clusters batch_size: 256
max iterations: 8000
tolerance threshold: 0.001
regularizers.l1(10e-5): LSTM layer 2
iteration: 140
delta_label: 0.0
Clustering training time: 341.0137917995453
0      0.005097
1      0.005328
2      0.005631
3      0.005153
4      0.005171
         ...   
195    0.005092
196    0.005124
197    0.005147
198    0.005097
199    0.005182
Length: 200, dtype: float64
35     896
0      806
170    684
22     415
69     405
      ... 
2        6
182      5
193      4
129      4
153      3
Name: 200, Length: 200, dtype: int64
