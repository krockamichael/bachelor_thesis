encoder layers: non-trainable
LSTM: 2
neurons_1: 128
neurons_2: 64.0
neurons_3: 10
epochs: 200
autoencoder batch_size: 128
n_clusters: 10
clusters batch_size: 256
max iterations: 8000
tolerance threshold: 0.001
regularizers.l1(10e-5): LSTM layer 2
Autoencoder training time: 4036.955159400001
Clustering training time: 2907.4707148075104
0    0.106642
1    0.139796
2    0.107224
3    0.106609
4    0.138546
5    0.139905
6    0.106479
7    0.107182
8    0.135845
9    0.106603
dtype: float64
5    1435
2     352
0     156
Name: 10, dtype: int64
