encoder layers: non-trainable
LSTM: 3
neurons_1: 128
neurons_2: 64.0
neurons_3: 10
epochs: 20
autoencoder batch_size: 128
n_clusters: 7
clusters batch_size: 256
max iterations: 8000
tolerance threshold: 0.001
regularizers.l1(10e-5): LSTM layer 1
Autoencoder training time: 5416.4262884
Clustering training time: 2095.005485534668
0    0.145260
1    0.156399
2    0.146914
3    0.145061
4    0.156175
5    0.155065
6    0.145205
dtype: float64
0    11062
1     2517
5     2191
2     1697
3      781
Name: 7, dtype: int64
