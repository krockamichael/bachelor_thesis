encoder layers: non-trainable
LSTM: 2
neurons_1: 128
neurons_2: 64.0
epochs: 20
autoencoder batch_size: 128
n_clusters: 7
clusters batch_size: 256
max iterations: 8000
tolerance threshold: 0.001
regularizers.l1(10e-5): LSTM layer 2
Autoencoder training time: 3955.3303279
iteration: 140
delta_label: 0.0
Clustering training time: 428.1943736076355
0    0.143793
1    0.143335
2    0.144029
3    0.143435
4    0.145748
5    0.143467
6    0.144009
dtype: float64
3    6662
2    2913
0    2367
5    2181
6    1966
1    1831
4     328
Name: 7, dtype: int64
