encoder layers: non-trainable
LSTM: 2
neurons_1: 128
neurons_2: 64.0
epochs: 20
autoencoder batch_size: 128
n_clusters: 10
clusters batch_size: 256
max iterations: 8000
tolerance threshold: 0.001
regularizers.l1(10e-5): LSTM layer 2
iteration: 140
delta_label: 0.0
iteration: 280
delta_label: 0.0012604120999561595
Clustering training time: 500.7750291824341
0    0.102250
1    0.100312
2    0.100353
3    0.100546
4    0.101101
5    0.100403
6    0.100541
7    0.100552
8    0.100797
9    0.100570
dtype: float64
1    6090
9    2394
3    1862
2    1784
5    1772
4    1653
8    1094
6     659
7     603
0     337
Name: 10, dtype: int64
